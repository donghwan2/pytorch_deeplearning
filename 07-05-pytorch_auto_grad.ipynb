{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Autograd : 파이토치에서 자동으로 미분을 해주는 기능 -> 나중에 loss 함수에 적용하면 가중치를 업데이트하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch AutoGrad 기능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]], requires_grad=True)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1, 2],  # (모델 파라미터로 예측한) 예측값\n",
    "                       [3, 4]]).requires_grad_(True) # 미분에 참여할 것인가? True (Loss를 미분할 예정)\n",
    "x  # 나중에 x로 미분할 예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# element-wise 연산\n",
    "x1 = x + 2\n",
    "x2 = x - 2\n",
    "x3 = x1 * x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 4.],\n",
      "        [5., 6.]], grad_fn=<AddBackward0>)\n",
      "tensor([[-1.,  0.],\n",
      "        [ 1.,  2.]], grad_fn=<SubBackward0>)\n",
      "tensor([[-3.,  0.],\n",
      "        [ 5., 12.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(x1)\n",
    "print(x2)\n",
    "print(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y(훗날 loss) ->  y는 x에 대해 미분 가능하다.\n",
    "y = x3.sum()   # y는 scalar\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# y를 x에 대해 미분하기 전 x 요소들의 기울기를 살펴보면,\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y 미분 실행   -> 결과: x.grad, 2x2의 기울기가 생긴다.\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-> 나중에 딥러닝 코드에서 loss 텐서(error tensor)에 .backward()를 호출하면 역전파가 시작한다는 의미이다.\n",
    "\n",
    "loss = (prediction - labels).sum()\n",
    "loss.backward()   # 역전파 단계(backward pass)\n",
    "\n",
    "https://tutorials.pytorch.kr/beginner/blitz/autograd_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{gathered}\n",
    "x=\\begin{bmatrix}\n",
    "x_{(1,1)} & x_{(1,2)} \\\\\n",
    "x_{(2,1)} & x_{(2,2)}\n",
    "\\end{bmatrix}\\\\\n",
    "\\\\\n",
    "x_1=x+2 \\\\\n",
    "x_2=x-2 \\\\\n",
    "\\\\\n",
    "\\begin{aligned}\n",
    "x_3&=x_1\\times{x_2} \\\\\n",
    "&=(x+2)(x-2) \\\\\n",
    "&=x^2-4\n",
    "\\end{aligned} \\\\\n",
    "\\\\\n",
    "\\begin{aligned}\n",
    "y&=\\text{sum}(x_3) \\\\\n",
    "&=x_{3,(1,1)}+x_{3,(1,2)}+x_{3,(2,1)}+x_{3,(2,2)}\n",
    "\\end{aligned} \\\\\n",
    "\\\\\n",
    "\\text{x.grad}=\\begin{bmatrix}\n",
    "\\frac{\\partial{y}}{\\partial{x_{(1,1)}}} & \\frac{\\partial{y}}{\\partial{x_{(1,2)}}} \\\\\n",
    "\\frac{\\partial{y}}{\\partial{x_{(2,1)}}} & \\frac{\\partial{y}}{\\partial{x_{(2,2)}}}\n",
    "\\end{bmatrix} \\\\\n",
    "\\\\\n",
    "\\frac{∂y}{∂x}=2x\n",
    "\\end{gathered}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "# y를 x에 대해 미분한 x의 기울기(∂y/∂x = 2x)를 살펴보면,\n",
    "print(x.grad)  # 2x2 matrix 기울기가 생겼다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-60eabc589838>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# x3도 gradient를 가지고 있는 텐서 이므로 numpy 변환이 안된다. 먼저 detach 변환이 필요하다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "# x3도 gradient를 가지고 있는 텐서 이므로 numpy 변환이 안된다. 먼저 detach 변환이 필요하다.\n",
    "x3.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.,  0.],\n",
       "        [ 5., 12.]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.,  0.],\n",
       "       [ 5., 12.]], dtype=float32)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# detach : 미분 함수 타입에서 숫자 타입으로 변환해준다. \n",
    "# autograd가 연산할 때마다 computational graph를 그리는데 이것을 끊어주는 것. \n",
    "# 그래야 넘파이와의 산술연산이 가능해진다.\n",
    "x3.detach_().numpy()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x3가 gradient를 가지는 텐서에서 넘파이로 변환된 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
