{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "목표 : nn.Module 클래스를 상속받아서 만든 클래스로 Linear Layer(FC layer)를 구현해보자(학습 x)\n",
    "\n",
    "이 챕터에서 이해하고 넘어가야할 핵심 내용들 : 클래스, 상속, init, super, Linear layer(FC layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear Layer(FC layer) 함수로 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{gathered}\n",
    "y=x\\cdot{W}+b, \\\\\n",
    "\\text{where }x\\in\\mathbb{R}^{N\\times{n}}\\text{, }y\\in\\mathbb{R}^{N\\times{m}}. \\\\\n",
    "\\\\\n",
    "\\text{Thus, }W\\in\\mathbb{R}^{n\\times{m}}\\text{ and }b\\in\\mathbb{R}^m.\n",
    "\\end{gathered}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([2., 2.])\n"
     ]
    }
   ],
   "source": [
    "# W는 n x m 벡터\n",
    "W = torch.FloatTensor([[1, 2],   # 3x2  -> |X| = (N, 3),  |y|=(N,2)\n",
    "                       [3, 4],   # input dimension = 3\n",
    "                       [5, 6]])  # output dimension = 2\n",
    "\n",
    "b = torch.FloatTensor([2, 2])    # Nx2\n",
    "\n",
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "|x| = (N, 3)\n",
    "|y| = (N, 2) = |b|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print(W.size())\n",
    "print(b.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear layer 함수\n",
    "def linear(x, W, b):\n",
    "    y = torch.matmul(x, W) + b\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "# 입력값 x 선언\n",
    "x = torch.FloatTensor([[1, 1, 1],   # 4x3\n",
    "                       [2, 2, 2],   # N=4\n",
    "                       [3, 3, 3],\n",
    "                       [4, 4, 4]])\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-> N=4\n",
    "\n",
    "∴ |y| = (4, 3) x (3, 2) = (4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11., 14.],\n",
      "        [20., 26.],\n",
      "        [29., 38.],\n",
      "        [38., 50.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 실제 y를 구해보자\n",
    "y = linear(x, W, b)\n",
    "\n",
    "print(y)\n",
    "y.size()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-> 더 쉽게 linear layer를 구성하는 방법이 없을까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. nn.Module 클래스 상속받아서 Linear layer 클래스 만들기"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import torch.nn as nn\n",
    "⦁ nn.Module 클래스를 상속해서 Neural network를 구현할 수 있다.\n",
    "  -> Neural network은 커스터마이징도 가능하다.  \n",
    "\n",
    "⦁ Linear 클래스 선언 시 두 개의 함수 __init__ 함수, forward 함수를 overide하게 된다.\n",
    "\n",
    "⦁ __init__ : 나중에 forward 함수에서 필요한 것들을 선언하는 공간\n",
    "\n",
    "⦁ forward 함수 : 실제 모델을 선언하는 공간"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (low level) nn.Module 상속 받아서 linear layer를 구현"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "사실 우리가 굳이 MyLinear 클래스를 구현 안 하고도 high level 코드(밑에 설명)인,\n",
    "nn.Linear 클래스로 간편하게 구현 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1210e-44, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00]])\n",
      "tensor([2.0000e+00, 7.3049e+03])\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 W, b는 랜덤으로 생성된다.\n",
    "W = torch.FloatTensor(3, 2)\n",
    "b = torch.FloatTensor(2)\n",
    "\n",
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# nn.Module 클래스를 상속받아서 Linear Layer(FC layer)을 수행하는 클래스 정의\n",
    "class MyLinear(nn.Module):  # nn.Module 상속\n",
    "    \n",
    "    def __init__(self, input_dim=3, output_dim=2):   # 객체 생성 시 input_dim, output_dim 정의\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        super().__init__()  # 상위 클래스의 init 메소드 호출\n",
    "        \n",
    "        # nn.Parameter : W, b가 nn.Module을 상속받은 객체의 파라미터라고 선언해준다.        \n",
    "        self.W = nn.Parameter(torch.FloatTensor(input_dim, output_dim))\n",
    "        self.b = nn.Parameter(torch.FloatTensor(output_dim))\n",
    "        \n",
    "    def forward(self, x):    # x : 입력 데이터  -> |x| = (batch_size, input_dim)\n",
    "        y = torch.matmul(x, self.W) + self.b\n",
    "        # |y| = (batch_size, input_dim) * (input_dim, output_dim) = (batch_size, output_dim)        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://pytorch.org/docs/stable/nn.html#torch.nn.Parameter\n",
    "\n",
    "A kind of Tensor that is to be considered a module parameter.\n",
    "\n",
    "Parameters are Tensor subclasses, that have a very special property when used with Module s - when they’re assigned as Module attributes they are automatically added to the list of its parameters, and will appear e.g. in parameters() iterator. Assigning a Tensor doesn’t have such effect. This is because one might want to cache some temporary state, like last hidden state of the RNN, in the model. If there was no such class as Parameter, these temporaries would get registered too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = MyLinear(3, 2)  \n",
    "\n",
    "y = linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0400e-32, 2.1684e-19],\n",
       "        [1.0400e-32, 4.3368e-19],\n",
       "        [1.0400e-32, 6.5052e-19],\n",
       "        [1.0400e-32, 8.6736e-19]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y.size())\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.0000e+00, 1.0842e-19],\n",
      "        [0.0000e+00, 1.0842e-19],\n",
      "        [9.8091e-45, 0.0000e+00]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1.0400e-32, 1.4013e-45], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# MyLinear 클래스 내에서 torch.FloatTensor에 의해 랜덤으로 생성된 파라미터 W, b 확인\n",
    "for p in linear.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-> 랜덤으로 생성된 파라미터 W, b 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (high level) nn.Linear 클래스 사용해서 간편하게 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Linear(input_dim, output_dim) 클래스로 객체(linear)를 생성하면,\n",
    "# 생성된 linear 객체에 자동으로 forward 함수가 매핑되어 있다(객체 선언 시 자동으로 실행된다.)\n",
    "# 인자로 forward 함수의 argument(입력 데이터)만 넣어주면 된다.\n",
    "linear = nn.Linear(3, 2)\n",
    "y = linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5792, -0.7592],\n",
       "        [-1.4116, -0.9707],\n",
       "        [-2.2440, -1.1821],\n",
       "        [-3.0765, -1.3935]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y.size())\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.4734, -0.2902, -0.0688],\n",
      "        [-0.1185,  0.2055, -0.2984]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.2533, -0.5478], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 랜덤 생성된 파라미터들 확인\n",
    "for p in linear.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Module can contain other nn.Module's child classes."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "MyLiear 클래스 안에서 nn.Linear 클래스를 사용할 수도 있음.\n",
    "(MyLiear 클래스는 그냥 껍데기만 있을 뿐)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=3, output_dim=2):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # |x| = (batch_size, input_dim)\n",
    "        y = self.linear(x)\n",
    "        # |y| = (batch_size, output_dim)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear layer 출력값 생성\n",
    "linear = MyLinear(3, 2)\n",
    "y = linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0333,  0.3377],\n",
       "        [-1.8432,  1.0565],\n",
       "        [-2.6531,  1.7753],\n",
       "        [-3.4630,  2.4941]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y.size())\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1130, -0.1795, -0.5174],\n",
      "        [ 0.2900, -0.0122,  0.4410]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2234, -0.3811], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 랜덤 생성된 파라미터들 확인\n",
    "for p in linear.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nn.Module을 상속받아서 만든 MyLinear 클래스 안에\n",
    "nn.Linear를 nn.Linear 클래스를 집어넣어서,\n",
    "linear layer를 구현해보았다."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "아직까지 모델 학습은 없는 상태. (그냥 선형 변환이라는 것을 구현해본 것뿐)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
