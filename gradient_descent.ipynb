{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Gradient Descent의 기본 원리 실습(low level)\n",
    "향후 실제 프로젝트를 할 때는 high level 코드(맨 아래 설명)로 구현할 예정."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 0.2000, 0.3000],\n",
       "        [0.4000, 0.5000, 0.6000],\n",
       "        [0.7000, 0.8000, 0.9000]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target값 -> 실제값\n",
    "target = torch.FloatTensor([[0.1, 0.2, 0.3],\n",
    "                            [0.4, 0.5, 0.6],\n",
    "                            [0.7, 0.8, 0.9]])\n",
    "target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x -> (모델 파라미터로 예측한) 예측값\n",
    "x = torch.rand_like(target)  # 임의의 값을 세팅\n",
    "\n",
    "# This means the final scalar will be differentiate by x.\n",
    "x.requires_grad = True\n",
    "# You can get gradient of x, after differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3633, 0.4203, 0.4412],\n",
       "        [0.6838, 0.3091, 0.0767],\n",
       "        [0.0068, 0.9675, 0.9947]], requires_grad=True)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x  # 3x3 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1162, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 타깃값과 예측값의 오차를 구해보면,\n",
    "loss = F.mse_loss(x, target)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-> 현재 상태의 loss는 0.1162\n",
    "미분을 통해 이 loss를 점차 줄여나가고자 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th Loss: 7.0313e-02\n",
      "tensor([[0.3048, 0.3713, 0.4098],\n",
      "        [0.6208, 0.3515, 0.1930],\n",
      "        [0.1609, 0.9303, 0.9736]], requires_grad=True) \n",
      "\n",
      "2-th Loss: 4.2535e-02\n",
      "tensor([[0.2593, 0.3332, 0.3854],\n",
      "        [0.5717, 0.3845, 0.2834],\n",
      "        [0.2807, 0.9013, 0.9573]], requires_grad=True) \n",
      "\n",
      "3-th Loss: 2.5731e-02\n",
      "tensor([[0.2239, 0.3036, 0.3664],\n",
      "        [0.5336, 0.4102, 0.3538],\n",
      "        [0.3739, 0.8788, 0.9446]], requires_grad=True) \n",
      "\n",
      "4-th Loss: 1.5566e-02\n",
      "tensor([[0.1963, 0.2806, 0.3517],\n",
      "        [0.5039, 0.4301, 0.4085],\n",
      "        [0.4463, 0.8613, 0.9347]], requires_grad=True) \n",
      "\n",
      "5-th Loss: 9.4163e-03\n",
      "tensor([[0.1749, 0.2627, 0.3402],\n",
      "        [0.4808, 0.4457, 0.4511],\n",
      "        [0.5027, 0.8477, 0.9270]], requires_grad=True) \n",
      "\n",
      "6-th Loss: 5.6963e-03\n",
      "tensor([[0.1583, 0.2488, 0.3313],\n",
      "        [0.4628, 0.4577, 0.4842],\n",
      "        [0.5466, 0.8371, 0.9210]], requires_grad=True) \n",
      "\n",
      "7-th Loss: 3.4459e-03\n",
      "tensor([[0.1453, 0.2379, 0.3243],\n",
      "        [0.4489, 0.4671, 0.5099],\n",
      "        [0.5807, 0.8288, 0.9163]], requires_grad=True) \n",
      "\n",
      "8-th Loss: 2.0846e-03\n",
      "tensor([[0.1353, 0.2295, 0.3189],\n",
      "        [0.4380, 0.4744, 0.5299],\n",
      "        [0.6072, 0.8224, 0.9127]], requires_grad=True) \n",
      "\n",
      "9-th Loss: 1.2610e-03\n",
      "tensor([[0.1274, 0.2229, 0.3147],\n",
      "        [0.4296, 0.4801, 0.5455],\n",
      "        [0.6278, 0.8174, 0.9099]], requires_grad=True) \n",
      "\n",
      "10-th Loss: 7.6284e-04\n",
      "tensor([[0.1213, 0.2178, 0.3114],\n",
      "        [0.4230, 0.4845, 0.5576],\n",
      "        [0.6438, 0.8136, 0.9077]], requires_grad=True) \n",
      "\n",
      "11-th Loss: 4.6147e-04\n",
      "tensor([[0.1166, 0.2139, 0.3089],\n",
      "        [0.4179, 0.4880, 0.5670],\n",
      "        [0.6563, 0.8106, 0.9060]], requires_grad=True) \n",
      "\n",
      "12-th Loss: 2.7916e-04\n",
      "tensor([[0.1129, 0.2108, 0.3069],\n",
      "        [0.4139, 0.4906, 0.5744],\n",
      "        [0.6660, 0.8082, 0.9046]], requires_grad=True) \n",
      "\n",
      "13-th Loss: 1.6888e-04\n",
      "tensor([[0.1100, 0.2084, 0.3054],\n",
      "        [0.4108, 0.4927, 0.5801],\n",
      "        [0.6736, 0.8064, 0.9036]], requires_grad=True) \n",
      "\n",
      "14-th Loss: 1.0216e-04\n",
      "tensor([[0.1078, 0.2065, 0.3042],\n",
      "        [0.4084, 0.4943, 0.5845],\n",
      "        [0.6795, 0.8050, 0.9028]], requires_grad=True) \n",
      "\n",
      "15-th Loss: 6.1800e-05\n",
      "tensor([[0.1061, 0.2051, 0.3033],\n",
      "        [0.4065, 0.4956, 0.5879],\n",
      "        [0.6840, 0.8039, 0.9022]], requires_grad=True) \n",
      "\n",
      "16-th Loss: 3.7385e-05\n",
      "tensor([[0.1047, 0.2040, 0.3025],\n",
      "        [0.4051, 0.4966, 0.5906],\n",
      "        [0.6876, 0.8030, 0.9017]], requires_grad=True) \n",
      "\n",
      "17-th Loss: 2.2616e-05\n",
      "tensor([[0.1037, 0.2031, 0.3020],\n",
      "        [0.4040, 0.4973, 0.5927],\n",
      "        [0.6903, 0.8023, 0.9013]], requires_grad=True) \n",
      "\n",
      "18-th Loss: 1.3681e-05\n",
      "tensor([[0.1029, 0.2024, 0.3015],\n",
      "        [0.4031, 0.4979, 0.5943],\n",
      "        [0.6925, 0.8018, 0.9010]], requires_grad=True) \n",
      "\n",
      "19-th Loss: 8.2763e-06\n",
      "tensor([[0.1022, 0.2019, 0.3012],\n",
      "        [0.4024, 0.4984, 0.5956],\n",
      "        [0.6942, 0.8014, 0.9008]], requires_grad=True) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "threshold = 1e-5     # 10의 -5승보다 loss가 작을 때는 학습(미분)을 멈출 예정.\n",
    "learning_rate = 1.\n",
    "iter_cnt = 0\n",
    "\n",
    "while loss > threshold:  # loss가 threshold보다 클 동안 미분 반복.\n",
    "    iter_cnt += 1\n",
    "    \n",
    "    # loss는 위에서 구해놓았다.\n",
    "    loss.backward()      # loss 미분 -> 결과: x.grad, 3x3의 기울기가 생긴다.\n",
    "\n",
    "    x = x - learning_rate * x.grad   # x 업데이트(미분값에 학습률을 곱해서 뻬줌)\n",
    "        \n",
    "    # detach : 미분 함수 타입에서 숫자 타입으로 변환해준다. \n",
    "    # autograd가 연산할 때마다 computational graph를 그리는데 이것을 끊어주는 것. \n",
    "    # 그래야 넘파이와의 산술연산이 가능해진다.\n",
    "    x.detach_()\n",
    "    x.requires_grad_(True)\n",
    "    \n",
    "    # loss 계산 \n",
    "    loss = F.mse_loss(x, target)\n",
    "    \n",
    "    print('%d-th Loss: %.4e' % (iter_cnt, loss))\n",
    "    print(x, '\\n')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gradient descent : loss를 최소화하는 파라미터 찾기\n",
    "\n",
    "특정 데이터를 나타내는 가상의 목표 함수가 있고, 그 함수에 파라미터가 있다.\n",
    "우리는 임의의 파라미터를 세팅하고 loss 값을 줄이도록 파라미터를 조절해서,\n",
    "결국에는 가상의 목표 함수를 approximation해 나갈 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (high level) GD 실제 적용 예시"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "나중에는 간단한 코드로 Gradient Descent를 수행할 예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear 모델 정의\n",
    "import torch.nn as nn\n",
    "model = nn.Linear(5, 3)  # (input_dim, output_dim)\n",
    "\n",
    "# optim.SGD()을 활용해서 모델 파라미터들의 미분을 수행하고, 파라미터 업데이트를 수행한다.\n",
    "# -> Gradient Descent 수행\n",
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(model.parameters(),  lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
